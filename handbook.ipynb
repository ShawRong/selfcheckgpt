{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#3 mode\n",
    "selfcheck_mqag = SelfCheckMQAG(device=device) # set device to 'cuda' if GPU is available\n",
    "\n",
    "#selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)\n",
    "#\n",
    "#selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.\n",
    "\n",
    "# LLM's text (e.g. GPT-3 response) to be evaluated at the sentence level  & Split it into sentences\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "question = 'What did Petra van Staveren win a gold medal for?'\n",
    "answer = 'Petra van Stoveren won silver medal in the 2008 Summer Olympics in Beijing, China.'\n",
    "sentences = [sent.text.strip() for sent in nlp(answer).sents] # spacy sentence tokenization\n",
    "print(sentences)\n",
    "\n",
    "# Other samples generated by the same LLM to perform self-check for consistency\n",
    "sample1 = \"Michael Alan Weiner (born March 31, 1942) is an American radio host. He is the host of The Savage Country.\"\n",
    "sample2 = \"Michael Alan Weiner (born January 13, 1960) is a Canadian radio host. He works at The New York Times.\"\n",
    "sample3 = \"Michael Alan Weiner (born March 31, 1942) is an American radio host. He obtained his PhD from MIT.\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "# SelfCheck-MQAG: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual\n",
    "# Additional params for each scoring_method:\n",
    "# -> counting: AT (answerability threshold, i.e. questions with answerability_score < AT are rejected)\n",
    "# -> bayes: AT, beta1, beta2\n",
    "# -> bayes_with_alpha: beta1, beta2\n",
    "sent_scores_mqag = selfcheck_mqag.predict(\n",
    "    sentences = sentences,               # list of sentences\n",
    "    passage = passage,                   # passage (before sentence-split)\n",
    "    sampled_passages = [sample1, sample2, sample3], # list of sampled passages\n",
    "    num_questions_per_sent = 5,          # number of questions to be drawn  \n",
    "    scoring_method = 'bayes_with_alpha', # options = 'counting', 'bayes', 'bayes_with_alpha'\n",
    "    beta1 = 0.8, beta2 = 0.8,            # additional params depending on scoring_method\n",
    ")\n",
    "print(sent_scores_mqag)\n",
    "# [0.30990949 0.42376232]\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "# SelfCheck-BERTScore: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual\n",
    "sent_scores_bertscore = selfcheck_bertscore.predict(\n",
    "    sentences = sentences,                          # list of sentences\n",
    "    sampled_passages = [sample1, sample2, sample3], # list of sampled passages\n",
    ")\n",
    "print(sent_scores_bertscore)\n",
    "# [0.0695562  0.45590915]\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------- #\n",
    "# SelfCheck-Ngram: Score at sentence- and document-level where value is in [0.0, +inf) and high value means non-factual\n",
    "# as opposed to SelfCheck-MQAG and SelfCheck-BERTScore, SelfCheck-Ngram's score is not bounded\n",
    "sent_scores_ngram = selfcheck_ngram.predict(\n",
    "    sentences = sentences,   \n",
    "    passage = passage,\n",
    "    sampled_passages = [sample1, sample2, sample3],\n",
    ")\n",
    "print(sent_scores_ngram)\n",
    "# {'sent_level': { # sentence-level score similar to MQAG and BERTScore variant\n",
    "#     'avg_neg_logprob': [3.184312, 3.279774],\n",
    "#     'max_neg_logprob': [3.476098, 4.574710]\n",
    "#     },\n",
    "#  'doc_level': {  # document-level score such that avg_neg_logprob is computed over all tokens\n",
    "#     'avg_neg_logprob': 3.218678904916201,\n",
    "#     'avg_max_neg_logprob': 4.025404834169327\n",
    "#     }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
