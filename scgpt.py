from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG, SelfCheckBERTScore, SelfCheckNgram
import torch
import spacy

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#3 mode
selfcheck_mqag = SelfCheckMQAG(device=device) # set device to 'cuda' if GPU is available

#selfcheck_bertscore = SelfCheckBERTScore(rescale_with_baseline=True)
#
#selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.

# LLM's text (e.g. GPT-3 response) to be evaluated at the sentence level  & Split it into sentences
nlp = spacy.load("en_core_web_sm")

question = 'What did Petra van Staveren win a gold medal for?'
answer = 'Petra van Stoveren won silver medal in the 2008 Summer Olympics in Beijing, China.'
passage = question + answer
sentences = [sent.text.strip() for sent in nlp(answer).sents] # spacy sentence tokenization
print(sentences)

# Other samples generated by the same LLM to perform self-check for consistency
sample1 = "Petra van Staveren won two gold medals at the 1987 Summer Olympic games in Seoul, South Korea - one in women's 100m sprint and one in women's 100m freestyle"
sample2 = "Petra van Staveren won a gold medal for women's individual vault during the 2007 European Championship Gymnastics in Moscow, Russia?."
sample3 = "Petra van Staveren won a gold medal in judo at the 2017 Summer Olympic Games in South Korea."
# --------------------------------------------------------------------------------------------------------------- #
# SelfCheck-MQAG: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
# Additional params for each scoring_method:
# -> counting: AT (answerability threshold, i.e. questions with answerability_score < AT are rejected)
# -> bayes: AT, beta1, beta2
# -> bayes_with_alpha: beta1, beta2
sent_scores_mqag = selfcheck_mqag.predict(
    sentences = sentences,               # list of sentences
    passage = passage,                   # passage (before sentence-split)
    sampled_passages = [sample1, sample2, sample3], # list of sampled passages
    num_questions_per_sent = 5,          # number of questions to be drawn  
    scoring_method = 'bayes_with_alpha', # options = 'counting', 'bayes', 'bayes_with_alpha'
    beta1 = 0.8, beta2 = 0.8,            # additional params depending on scoring_method
)
print(sent_scores_mqag)
# [0.30990949 0.42376232]

# --------------------------------------------------------------------------------------------------------------- #
# SelfCheck-BERTScore: Score for each sentence where value is in [0.0, 1.0] and high value means non-factual
#sent_scores_bertscore = selfcheck_bertscore.predict(
    #sentences = sentences,                          # list of sentences
    #sampled_passages = [sample1, sample2, sample3], # list of sampled passages
#)
#print(sent_scores_bertscore)
# [0.0695562  0.45590915]

# --------------------------------------------------------------------------------------------------------------- #
# SelfCheck-Ngram: Score at sentence- and document-level where value is in [0.0, +inf) and high value means non-factual
# as opposed to SelfCheck-MQAG and SelfCheck-BERTScore, SelfCheck-Ngram's score is not bounded
#sent_scores_ngram = selfcheck_ngram.predict(
    #sentences = sentences,   
    #passage = passage,
    #sampled_passages = [sample1, sample2, sample3],
#)
#print(sent_scores_ngram)
# {'sent_level': { # sentence-level score similar to MQAG and BERTScore variant
#     'avg_neg_logprob': [3.184312, 3.279774],
#     'max_neg_logprob': [3.476098, 4.574710]
#     },
#  'doc_level': {  # document-level score such that avg_neg_logprob is computed over all tokens
#     'avg_neg_logprob': 3.218678904916201,
#     'avg_max_neg_logprob': 4.025404834169327
#     }
# }


