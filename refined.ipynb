{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srongaa/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/srongaa/llm/selfcheckgpt/myselfcheckgpt/modeling_mqag.py:63: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  distractors = re.sub(\"<extra\\S+>\", g2_tokenizer.sep_token, distractors)\n",
      "/home/srongaa/llm/selfcheckgpt/myselfcheckgpt/modeling_mqag.py:130: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  distractors = re.sub(\"<extra\\S+>\", g2_tokenizer.sep_token, distractors)\n",
      "/home/srongaa/llm/selfcheckgpt/myselfcheckgpt/modeling_mqag.py:194: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  distractors = re.sub(\"<extra\\S+>\", g2_tokenizer.sep_token, distractors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfCheck-1gram initialized\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from myselfcheckgpt.modeling_selfcheck import SelfCheckNgram\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "selfcheck_ngram = SelfCheckNgram(n=1) # n=1 means Unigram, n=2 means Bigram, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "input_file_name = \"./input/en.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_file_jsonl(file :str):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file, mode='r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Parse each line as a JSON object\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format. {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_file_jsonl(input_file_name)\n",
    "print(data[0]['model_input'])\n",
    "print(data[0]['model_output_text'])\n",
    "model_name = data[0]['model_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you. I've been busy with work and family, but I have a few minutes to catch up on things. How about you?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(message_to_ask, model):\n",
    "    #question here\n",
    "    message = message_to_ask\n",
    "\n",
    "    config = dict(top_k=50, top_p=0.90, temperature=0.1)\n",
    "    prompt = message + '\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # eos_token_id=tokenizer.encode('\\n'),\n",
    "    # pad_token_id=tokenizer.encode('\\n')[0],\n",
    "    **config,\n",
    "    )\n",
    "\n",
    "    response_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    response_text = response_text.replace(prompt, \"\") # response repeats the input in the begining\n",
    "    response_token_ids = outputs.sequences[0].to(\"cpu\").tolist()[len(inputs.input_ids[0]):]\n",
    "    # response_embeddings = outputs.sequences[0].to(\"cpu\").tolist()[len(inputs.input_ids[0]):]\n",
    "    #response_tokens = tokenizer.convert_ids_to_tokens(response_token_ids)\n",
    "    #response_logits = [l.to(\"cpu\").tolist() for l in outputs.logits]\n",
    "\n",
    "    return response_text\n",
    "\n",
    "ask('Hello, how are you going?', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences ['Die spanische Provinz heißt Álava.']\n",
      "passage In welcher Provinz liegt die spanische Gemeinde Cilleruelo de San Mamés?\n",
      "\n",
      "Die spanische Provinz heißt Álava.\n",
      "sample_lst ['Cillerulo is a municipality located in the province of San Sebastian, in Gipuzkoa, Spain.', 'Cillerulo is a municipality located in the province of San Sebastian, in Gipuzkoa, Spain.', 'Cillerulo de san Mamé is located in the province of San Sebastian, in northern Spain.']\n",
      "test\n",
      "[Die, spanische, Provinz, heißt, Álava, .]\n",
      "[3.54095932 3.54095932 3.54095932 4.2341065  4.2341065  2.84781214]\n",
      "sentences ['Der Film \"\\nBrats\\n– Ein \\nDorf macht  Schule\" gewann den Publikumspreis beim  Internationalen  Dokumentarfilmfestival', 'München  2019.']\n",
      "passage Welchen Preis gewann der Dokumentarfilm \"Bratsch – Ein Dorf macht Schule\"?\n",
      "\n",
      "Der Film \"\n",
      "Brats\n",
      "– Ein \n",
      "Dorf macht  Schule\" gewann den Publikumspreis beim  Internationalen  Dokumentarfilmfestival  München  2019.\n",
      "sample_lst [\"Der Dokumentary-Film „Bratch - Ein dorf macht Schule“ hat den Preis “Best Documentary” gewonnen.\\nThe documentary film ‘Brash — A Village School’ has won the 'Best documentary' award at the International Film Festival in Oberhausen. The film was also awarded the Best Documentary Award at this year's Berlinale.\", 'Der Dokumentary-Film “Bratch - Ein dorf macht Schule” hat den Preis für den besten Dokumentären gewonnen.\\nThe documentary film „Brash — A village makes school“ has won the award for the best documentary.', \"Der Dokumentary-Film „Bratch - Ein dorf macht Schule“ hat den Preis “Best Documentary” gewonnen.\\nThe documentary film ‘Brash — A Village School’ has won the 'Best documentary' award at the International Film Festival in Berlin. The film was made by the German director Thomas Heise.\"]\n",
      "test\n",
      "[Der, Film, \", \n",
      ", Brats, \n",
      ", –, Ein, \n",
      ", Dorf, macht,  , Schule, \", gewann, den, Publikumspreis, beim,  , Internationalen,  , Dokumentarfilmfestival, München,  , 2019, .]\n",
      "[3.69882978 2.91037242 3.92197334 4.20965541 5.3082677  4.20965541\n",
      " 4.61512052 3.69882978 4.20965541 3.69882978 3.69882978 3.92197334\n",
      " 3.69882978 3.92197334 4.61512052 3.69882978 5.3082677  5.3082677\n",
      " 3.92197334 5.3082677  3.92197334 5.3082677  5.3082677  3.92197334\n",
      " 5.3082677  3.11104312]\n",
      "sentences ['Die Drosseln wiegen zwischen 200 und 300 Gramm.']\n",
      "passage Wie viel wiegt eine Drosselkrähe?\n",
      "Die Drosseln wiegen zwischen 200 und 300 Gramm.\n",
      "sample_lst [\"I'm sorry, I cannot provide an accurate answer to that question as it is not a specific term or phrase.\", \"I'm sorry, I don't understand the question. Can you please provide more context or rephrase the query?\", \"I'm sorry, I don't understand the question. Can you please provide more context or rephrase the query?\"]\n",
      "test\n",
      "[Die, Drosseln, wiegen, zwischen, 200, und, 300, Gramm, .]\n",
      "[4.4308168  4.4308168  4.4308168  4.4308168  4.4308168  4.4308168\n",
      " 4.4308168  4.4308168  3.04452244]\n",
      "sentences ['Ist das eine Abkürzung für \"Internationale Organisation der Migranten\"?']\n",
      "passage Wofür steht die Abkürzung IOMMU?\n",
      "\n",
      "Ist das eine Abkürzung für \"Internationale Organisation der Migranten\"?\n",
      "sample_lst ['The abbreviation IOOMU stands for the International Organization of Oil and Natural Gas Unions. It is an international trade union representing workers in the oil and gas industry.', 'The abbreviation IOOMU stands for the International Organization of Oil and Natural Gas Unions. It is an international trade union representing workers in the oil and gas industry.', 'The abbreviation IOOMU stands for the International Organization of Oil and Natural Gas Unions. It is an international trade union representing workers in the oil and natural gas industry.']\n",
      "test\n",
      "[Ist, das, eine, Abkürzung, für, \", Internationale, Organisation, der, Migranten, \", ?]\n",
      "[4.69134788 4.69134788 4.69134788 3.9982007  4.69134788 3.9982007\n",
      " 4.69134788 4.69134788 4.69134788 4.69134788 3.9982007  3.9982007 ]\n",
      "sentences ['Der kazachischen Verein FC Astna-69 gewann seinen 2.', 'Pokal gegen den FC Taraz.', '<|im_end|>']\n",
      "passage Gegen welchen Verein gewann der kasachische Verein FK Astana-1964 seinen zweiten Pokalsieg?\n",
      "Der kazachischen Verein FC Astna-69 gewann seinen 2. Pokal gegen den FC Taraz.\n",
      "<|im_end|>  \n",
      "\n",
      "sample_lst ['Der Verein Astanazan gewannen den zweiten pokal in Folge.', 'Der Verein Astanazan gewannen den zweiten pokal in Folge.', 'Der Verein Astanazan gewannen den zweiten pokal in Folge.']\n",
      "test\n",
      "[Der, kazachischen, Verein, FC, Astna-69, gewann, seinen, 2, ., Pokal, gegen, den, FC, Taraz, ., <, |im_end|, >]\n",
      "[2.50143595 4.11087386 2.31911439 3.41772668 4.11087386 3.41772668\n",
      " 3.41772668 4.11087386 2.50143595 2.7245795  3.41772668 2.7245795\n",
      " 3.41772668 4.11087386 2.50143595 4.11087386 4.11087386 4.11087386]\n",
      "sentences ['Die 100 Meter Brust gewann bei Olympia 1984 die US-Amerikanerin Barbara Frale.']\n",
      "passage Wer gewann die 100m Brust der Damen bei den Olympischen Spielen 1984?\n",
      "\n",
      "Die 100 Meter Brust gewann bei Olympia 1984 die US-Amerikanerin Barbara Frale.\n",
      "sample_lst [\"I'm sorry, I cannot provide a specific answer as the prompt is unclear. Can you please provide more context or rephrase the question?\", \"I'm sorry, I cannot provide a specific answer as the prompt is missing information.\", \"I'm sorry, I cannot provide a specific answer as the prompt does not provide enough context.\"]\n",
      "test\n",
      "[Die, 100, Meter, Brust, gewann, bei, Olympia, 1984, die, US, -, Amerikanerin, Barbara, Frale, .]\n",
      "[3.4552646  3.86072971 4.55387689 3.86072971 3.86072971 3.86072971\n",
      " 4.55387689 3.86072971 3.4552646  4.55387689 4.55387689 4.55387689\n",
      " 4.55387689 4.55387689 3.16758253]\n"
     ]
    }
   ],
   "source": [
    "to_save = []\n",
    "for item in data:\n",
    "    question = item['model_input']\n",
    "    answer = item['model_output_text']\n",
    "    sample_lst = []\n",
    "    for i in range(3):\n",
    "        sample = ask(question, model)\n",
    "        sample_lst.append(sample)\n",
    "\n",
    "    sample_withquestion = []\n",
    "    for sample in sample_lst:\n",
    "        sample_withquestion.append(question + '\\n' + sample)\n",
    "    passage = question + '\\n' + answer\n",
    "    #print(sample_withquestion)\n",
    "\n",
    "    to_eval = answer.strip() \n",
    "    doc = nlp(to_eval)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents] # spacy sentence tokenization\n",
    "    print('sentences', sentences)\n",
    "    print('passage', passage)\n",
    "    print('sample_lst', sample_lst)\n",
    "    sent_scores_ngram = selfcheck_ngram.predict(\n",
    "        sentences = sentences,   \n",
    "        passage = passage,\n",
    "        sampled_passages = sample_lst,\n",
    "    )\n",
    "    #print(sent_scores_ngram)\n",
    "    lst_logprob = sent_scores_ngram['sent_level']['lst_neg_logprob']\n",
    "    lst_logprob = np.abs([item for sublist in lst_logprob for item in sublist])\n",
    "\n",
    "    doc = nlp(to_eval)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents] # spacy sentence tokenization\n",
    "    token_lst = []\n",
    "    for sent in doc.sents:\n",
    "        for token in nlp(sent.text.strip()):\n",
    "            token_lst.append(token)\n",
    "    print(token_lst)\n",
    "    print(lst_logprob)\n",
    "    df = pd.DataFrame({'tokens': token_lst, 'logprob': lst_logprob})\n",
    "    to_drop = df[df['logprob'] == df['logprob'].max()]['tokens'].tolist()\n",
    "    to_save.append(to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[name, for, line, produced, by, Diageo, first, introduced, United, States, 1945, However, image, as, pirate, with, parrot, on, his, shoulder, did, appear, until, 1954, origin, actual, blend, well, documented, and, there, no, evidence, to, suggest, that, it, before, 's, introduction], [a, (, ), It, third, -, largest, after, Yekaterinburg, Nizhniy, known, for, its, heavy, industry, particularly, metallurgy, machine, building], [distinct, historical, although, closely, related, often, associated, each, historic, at, heart, has, main, rulers, since, 14th, various, palaces, buildings, surrounded, by, thick, walls, towers, making, formidable, on, hand, original, core, built, late, 13th, Grand, Prince, his, court, means, citadel, or, smaller, more, compact, area, within, Square, famous, such, St., Basil, 's, Archangel, Michael, So, while, both, complexes, have, residential, functions, that, one, parts], [plays, Douglas, which, is, located, South, Lanarkshire, Scotland, capacity, around, 6,000, and, been, ground, since, 1878, making, it, one, oldest, grounds, world, still, use, by, single], [2004, It, formed, through, merger, two, neighboring, communes, and, as, part, a, wave, municipal, consolidations, in, Switzerland], [also, known, as, FIDE, (, Fédération, Internationale, des, Échecs, ), not, any, one, specific, an, international, organization, with, its, headquarters, currently, located, Minsk, Belarus, However, maintains, offices, several, countries, and, holds, various, events, around], [apologize, any, confusion, but, seems, be, a, missing, your, are, indeed, two, located, canton, of, Graubünden, Switzerland, However, into, one, single, municipality, have, remained, separate, entities, up, until, now, Therefore, is, no, as, this], [(, ), updated, Clarivate, Analytics, part, Web, Core, Collection, which, also, includes, such, Expanded, Social, Sciences, covers, a, wide, range, disciplines, in, including, archaeology, area, art, classical, history, literature, philosophy, religion, linguistics, provides, access, to, current, retrospective, information, from, thousands, scholarly, journals, conference, proceedings, books, sources], [television, which, aired, from, 2009, to, 2014, there, specific, character, named, However, main, protagonist, Caffrey, Matt, Bomer, former, criminal, art, forger, works, consultant, for, solving, agent, charge, overseeing, work, Tim, DeKay, So, while, per, se, he, plays, significant, role, managing, coordinating, investigations], [possible, identify, single, likely, recognized, distinct, entity, much, later, group, disorders, affect, muscle, coordination, movement, often, caused, damage, developing, brain, before, during, birth, term, used, 1845, Dr., John, Hinton, but, 20th, centuries, better, understood, diagnosed, treatments, focused, on, managing, symptoms, through, physical, orthopedic, interventions, use, braces, other, assistive, devices], [people, make, up, vast, majority, population, an, ethno, linguistic, predominantly, found, Northern, neighboring, countries, primarily, farmers, traders, herders], [not, a, well, or, established, settlement, around, inhabited, various, tribes, including, Seminoles, first, European, settlers, arrived, mid-1800s, community, began, develop, after, completion, Tampa, Hillsborough, Railroad, 1885, Therefore, there, is, no, recorded, history], [medium, sized, bird, with, an, average, 15.3, wingspan, 20.3, 8, weigh, between, 19, 34, grams, 0.7, 1.2, ounces, Their, bill, long, curved, measuring, about, 3.5, 1.4], [an, internal, by, Microsoft, aimed, merge, user, interfaces, XP, and, Tablet, PC, Edition, single, later, merged, with, another, called, eventually, became, neither, nor, standalone, products, ;, instead, their, features, incorporated, in, January, 2007], [did, write, entire, on, his, own, original, storyline, which, published, issues, #, 141, 142, Uncanny, 1981, illustrated, John, Byrne, However, alternative, sequences, were, drawn, Terry, Austin, artists, contributed, later, adaptations, this, into, media, formats, like, animation, film, So, while, role, significant, it, important, acknowledge, contributions, creators, involved, bringing, life], [also, known, as, Thorn, German, member, approximately, 1260, expulsion, 1524, commercial, defensive, confederation, merchant, guilds, market, towns, Northwestern, Central, Europe, 's, membership, contributed, significantly, economic, growth, during, Middle, Ages], [Pposey], [Zenit-2, Saint, Petersburg], [Häfen], [sides, Sect, Heretics], [Octobre], [1625], [Eduardo, Mango], [role, Zack, first, season, Scary, Movie, 5], [where, cases, who, have, and, researcher, will, compare, with, same, type, from], [MONK, who, serves, as, sect, He, also, current, lama, SakYABooks.com], [Jefs, Rakins, died, on, March, 1, ,, 2011, He, was, 88, years, old], [emperor, Augustus, at, Lugudunon, 43, BC], [Bishofshain, constituent, early], [Lahm], [Bernhard, Lönneber, L, son, Bernh, m, born, in, Stockholm, Sweden, March, 4, 1786], [Ponza]]\n",
      "[[line, produced, Diageo, introduced, United, States, 1945, image, pirate, parrot, shoulder, appear, 1954, origin, actual, blend, documented, evidence, suggest, introduction], [(, ), -, largest, Yekaterinburg, Nizhniy, known, heavy, industry, particularly, metallurgy, machine, building], [distinct, historical, closely, related, associated, historic, heart, main, rulers, 14th, palaces, buildings, surrounded, thick, walls, towers, making, formidable, hand, original, core, built, late, 13th, Grand, Prince, court, means, citadel, smaller, compact, area, Square, famous, St., Basil, Archangel, Michael, complexes, residential, functions, parts], [plays, Douglas, located, South, Lanarkshire, Scotland, capacity, 6,000, ground, 1878, making, oldest, grounds, world, use, single], [2004, formed, merger, neighboring, communes, wave, municipal, consolidations, Switzerland], [known, FIDE, (, Fédération, Internationale, des, Échecs, ), specific, international, organization, headquarters, currently, located, Minsk, Belarus, maintains, offices, countries, holds, events], [apologize, confusion, missing, located, canton, Graubünden, Switzerland, single, municipality, remained, separate, entities], [(, ), updated, Clarivate, Analytics, Web, Core, Collection, includes, Expanded, Social, Sciences, covers, wide, range, disciplines, including, archaeology, area, art, classical, history, literature, philosophy, religion, linguistics, provides, access, current, retrospective, information, thousands, scholarly, journals, conference, proceedings, books, sources], [television, aired, 2009, 2014, specific, character, named, main, protagonist, Caffrey, Matt, Bomer, criminal, art, forger, works, consultant, solving, agent, charge, overseeing, work, Tim, DeKay, se, plays, significant, role, managing, coordinating, investigations], [possible, identify, single, likely, recognized, distinct, entity, later, group, disorders, affect, muscle, coordination, movement, caused, damage, developing, brain, birth, term, 1845, Dr., John, Hinton, 20th, centuries, better, understood, diagnosed, treatments, focused, managing, symptoms, physical, orthopedic, interventions, use, braces, assistive, devices], [people, vast, majority, population, ethno, linguistic, predominantly, found, Northern, neighboring, countries, primarily, farmers, traders, herders], [established, settlement, inhabited, tribes, including, Seminoles, European, settlers, arrived, mid-1800s, community, began, develop, completion, Tampa, Hillsborough, Railroad, 1885, recorded, history], [medium, sized, bird, average, 15.3, wingspan, 20.3, 8, weigh, 19, 34, grams, 0.7, 1.2, ounces, bill, long, curved, measuring, 3.5, 1.4], [internal, Microsoft, aimed, merge, user, interfaces, XP, Tablet, PC, Edition, single, later, merged, called, eventually, standalone, products, ;, instead, features, incorporated, January, 2007], [write, entire, original, storyline, published, issues, #, 141, 142, Uncanny, 1981, illustrated, John, Byrne, alternative, sequences, drawn, Terry, Austin, artists, contributed, later, adaptations, media, formats, like, animation, film, role, significant, important, acknowledge, contributions, creators, involved, bringing, life], [known, Thorn, German, member, approximately, 1260, expulsion, 1524, commercial, defensive, confederation, merchant, guilds, market, towns, Northwestern, Central, Europe, membership, contributed, significantly, economic, growth, Middle, Ages], [Pposey], [Zenit-2, Saint, Petersburg], [Häfen], [sides, Sect, Heretics], [Octobre], [1625], [Eduardo, Mango], [role, Zack, season, Scary, Movie, 5], [cases, researcher, compare, type], [MONK, serves, sect, current, lama, SakYABooks.com], [Jefs, Rakins, died, March, 1, ,, 2011, 88, years, old], [emperor, Augustus, Lugudunon, 43, BC], [Bishofshain, constituent, early], [Lahm], [Bernhard, Lönneber, L, son, Bernh, m, born, Stockholm, Sweden, March, 4, 1786], [Ponza]]\n"
     ]
    }
   ],
   "source": [
    "print(to_save)\n",
    "filtered_list = [[word for word in item if not word.is_stop] for item in to_save ]\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['line', 'produced', 'Diageo', 'introduced', 'United', 'States', '1945', 'image', 'pirate', 'parrot', 'shoulder', 'appear', '1954', 'origin', 'actual', 'blend', 'documented', 'evidence', 'suggest', 'introduction'], ['(', ')', '-', 'largest', 'Yekaterinburg', 'Nizhniy', 'known', 'heavy', 'industry', 'particularly', 'metallurgy', 'machine', 'building'], ['distinct', 'historical', 'closely', 'related', 'associated', 'historic', 'heart', 'main', 'rulers', '14th', 'palaces', 'buildings', 'surrounded', 'thick', 'walls', 'towers', 'making', 'formidable', 'hand', 'original', 'core', 'built', 'late', '13th', 'Grand', 'Prince', 'court', 'means', 'citadel', 'smaller', 'compact', 'area', 'Square', 'famous', 'St.', 'Basil', 'Archangel', 'Michael', 'complexes', 'residential', 'functions', 'parts'], ['plays', 'Douglas', 'located', 'South', 'Lanarkshire', 'Scotland', 'capacity', '6,000', 'ground', '1878', 'making', 'oldest', 'grounds', 'world', 'use', 'single'], ['2004', 'formed', 'merger', 'neighboring', 'communes', 'wave', 'municipal', 'consolidations', 'Switzerland'], ['known', 'FIDE', '(', 'Fédération', 'Internationale', 'des', 'Échecs', ')', 'specific', 'international', 'organization', 'headquarters', 'currently', 'located', 'Minsk', 'Belarus', 'maintains', 'offices', 'countries', 'holds', 'events'], ['apologize', 'confusion', 'missing', 'located', 'canton', 'Graubünden', 'Switzerland', 'single', 'municipality', 'remained', 'separate', 'entities'], ['(', ')', 'updated', 'Clarivate', 'Analytics', 'Web', 'Core', 'Collection', 'includes', 'Expanded', 'Social', 'Sciences', 'covers', 'wide', 'range', 'disciplines', 'including', 'archaeology', 'area', 'art', 'classical', 'history', 'literature', 'philosophy', 'religion', 'linguistics', 'provides', 'access', 'current', 'retrospective', 'information', 'thousands', 'scholarly', 'journals', 'conference', 'proceedings', 'books', 'sources'], ['television', 'aired', '2009', '2014', 'specific', 'character', 'named', 'main', 'protagonist', 'Caffrey', 'Matt', 'Bomer', 'criminal', 'art', 'forger', 'works', 'consultant', 'solving', 'agent', 'charge', 'overseeing', 'work', 'Tim', 'DeKay', 'se', 'plays', 'significant', 'role', 'managing', 'coordinating', 'investigations'], ['possible', 'identify', 'single', 'likely', 'recognized', 'distinct', 'entity', 'later', 'group', 'disorders', 'affect', 'muscle', 'coordination', 'movement', 'caused', 'damage', 'developing', 'brain', 'birth', 'term', '1845', 'Dr.', 'John', 'Hinton', '20th', 'centuries', 'better', 'understood', 'diagnosed', 'treatments', 'focused', 'managing', 'symptoms', 'physical', 'orthopedic', 'interventions', 'use', 'braces', 'assistive', 'devices'], ['people', 'vast', 'majority', 'population', 'ethno', 'linguistic', 'predominantly', 'found', 'Northern', 'neighboring', 'countries', 'primarily', 'farmers', 'traders', 'herders'], ['established', 'settlement', 'inhabited', 'tribes', 'including', 'Seminoles', 'European', 'settlers', 'arrived', 'mid-1800s', 'community', 'began', 'develop', 'completion', 'Tampa', 'Hillsborough', 'Railroad', '1885', 'recorded', 'history'], ['medium', 'sized', 'bird', 'average', '15.3', 'wingspan', '20.3', '8', 'weigh', '19', '34', 'grams', '0.7', '1.2', 'ounces', 'bill', 'long', 'curved', 'measuring', '3.5', '1.4'], ['internal', 'Microsoft', 'aimed', 'merge', 'user', 'interfaces', 'XP', 'Tablet', 'PC', 'Edition', 'single', 'later', 'merged', 'called', 'eventually', 'standalone', 'products', ';', 'instead', 'features', 'incorporated', 'January', '2007'], ['write', 'entire', 'original', 'storyline', 'published', 'issues', '#', '141', '142', 'Uncanny', '1981', 'illustrated', 'John', 'Byrne', 'alternative', 'sequences', 'drawn', 'Terry', 'Austin', 'artists', 'contributed', 'later', 'adaptations', 'media', 'formats', 'like', 'animation', 'film', 'role', 'significant', 'important', 'acknowledge', 'contributions', 'creators', 'involved', 'bringing', 'life'], ['known', 'Thorn', 'German', 'member', 'approximately', '1260', 'expulsion', '1524', 'commercial', 'defensive', 'confederation', 'merchant', 'guilds', 'market', 'towns', 'Northwestern', 'Central', 'Europe', 'membership', 'contributed', 'significantly', 'economic', 'growth', 'Middle', 'Ages'], ['Pposey'], ['Zenit-2', 'Saint', 'Petersburg'], ['Häfen'], ['sides', 'Sect', 'Heretics'], ['Octobre'], ['1625'], ['Eduardo', 'Mango'], ['role', 'Zack', 'season', 'Scary', 'Movie', '5'], ['cases', 'researcher', 'compare', 'type'], ['MONK', 'serves', 'sect', 'current', 'lama', 'SakYABooks.com'], ['Jefs', 'Rakins', 'died', 'March', '1', ',', '2011', '88', 'years', 'old'], ['emperor', 'Augustus', 'Lugudunon', '43', 'BC'], ['Bishofshain', 'constituent', 'early'], ['Lahm'], ['Bernhard', 'Lönneber', 'L', 'son', 'Bernh', 'm', 'born', 'Stockholm', 'Sweden', 'March', '4', '1786'], ['Ponza']]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "token_lst_lst = []\n",
    "for save in filtered_list:\n",
    "    token_lst = list(token.text for token in save)\n",
    "    token_lst_lst.append(token_lst)\n",
    "print(token_lst_lst)\n",
    "\n",
    "try:\n",
    "    with open(f'./output/output{timestamp}.json', 'w') as f:\n",
    "        json.dump(token_lst_lst, f, indent=4)\n",
    "except Exception as e:\n",
    "    print(\"发生错误:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
